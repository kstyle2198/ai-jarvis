{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explain me Minimum propulsion power in  commer...</td>\n",
       "      <td>OK, Captain. \\n\\nThe MEPC.1/Circ.850/Rev.3 gui...</td>\n",
       "      <td>['39 Refer to Guidelines for determining minim...</td>\n",
       "      <td>Minimum Propulsion Power (MPP) is the power re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>explain me how to judge the satisfaction of th...</td>\n",
       "      <td>OK, Captain. \\n\\nTo judge the satisfaction of ...</td>\n",
       "      <td>[None, '.3 reasons for other changes made in t...</td>\n",
       "      <td>If the Attained EEDI is below the Required EED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explain me the information of reduction factor...</td>\n",
       "      <td>OK, Captain. \\n\\nThe reduction factor in comme...</td>\n",
       "      <td>['85,000 GT\\\\n\\\\n* Reduction factor to be line...</td>\n",
       "      <td>The reduction factor is an indicator that show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>explain me how to calculate eedi in dual fuel ...</td>\n",
       "      <td>OK, Captain. \\n\\nCalculating the EEDI (Energy ...</td>\n",
       "      <td>['.7 calculated values of the attained EEDIwea...</td>\n",
       "      <td>In the case of a ship equipped with a dual-fue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explain me how to calculate attained eedi and ...</td>\n",
       "      <td>OK, Captain. \\n\\nCalculating the Attained EEDI...</td>\n",
       "      <td>[None, None, '* To be conducted by a test orga...</td>\n",
       "      <td>The subscripts ME(i) and AE(i) refer to the ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  explain me Minimum propulsion power in  commer...   \n",
       "1  explain me how to judge the satisfaction of th...   \n",
       "2  explain me the information of reduction factor...   \n",
       "3  explain me how to calculate eedi in dual fuel ...   \n",
       "4  explain me how to calculate attained eedi and ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  OK, Captain. \\n\\nThe MEPC.1/Circ.850/Rev.3 gui...   \n",
       "1  OK, Captain. \\n\\nTo judge the satisfaction of ...   \n",
       "2  OK, Captain. \\n\\nThe reduction factor in comme...   \n",
       "3  OK, Captain. \\n\\nCalculating the EEDI (Energy ...   \n",
       "4  OK, Captain. \\n\\nCalculating the Attained EEDI...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  ['39 Refer to Guidelines for determining minim...   \n",
       "1  [None, '.3 reasons for other changes made in t...   \n",
       "2  ['85,000 GT\\\\n\\\\n* Reduction factor to be line...   \n",
       "3  ['.7 calculated values of the attained EEDIwea...   \n",
       "4  [None, None, '* To be conducted by a test orga...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Minimum Propulsion Power (MPP) is the power re...  \n",
       "1  If the Attained EEDI is below the Required EED...  \n",
       "2  The reduction factor is an indicator that show...  \n",
       "3  In the case of a ship equipped with a dual-fue...  \n",
       "4  The subscripts ME(i) and AE(i) refer to the ma...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = pd.read_csv(\"../results/test_007.csv\")\n",
    "t_df = t_df[[\"Query\", \"Answer\", \"Context\", \"ground_truth\"]]\n",
    "t_df.columns = ['question', 'answer', 'contexts', 'ground_truth']\n",
    "# t_df['ground_truth'] = \"\"\n",
    "t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explain me how to calculate attained eedi and ...</td>\n",
       "      <td>OK, Captain. \\n\\nCalculating the Attained EEDI...</td>\n",
       "      <td>[[None, None, '* To be conducted by a test org...</td>\n",
       "      <td>The subscripts ME(i) and AE(i) refer to the ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "4  explain me how to calculate attained eedi and ...   \n",
       "\n",
       "                                              answer  \\\n",
       "4  OK, Captain. \\n\\nCalculating the Attained EEDI...   \n",
       "\n",
       "                                            contexts  \\\n",
       "4  [[None, None, '* To be conducted by a test org...   \n",
       "\n",
       "                                        ground_truth  \n",
       "4  The subscripts ME(i) and AE(i) refer to the ma...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df[\"contexts\"] = t_df[\"contexts\"].astype(str).apply(lambda x: [x] if x else [])\n",
    "\n",
    "\n",
    "t_df = t_df.iloc[4:5,:]\n",
    "t_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Huggingfase Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def pandas_to_ragas(df):\n",
    "    '''\n",
    "    Converts a Pandas DataFrame into a Ragas-compatible dataset\n",
    "    \n",
    "    Inputs:\n",
    "        - df (Pandas DataFrame): The input DataFrame to be converted\n",
    "        \n",
    "    Returns:\n",
    "        - ragas_testset (Hugging Face Dataset): A Hugging Face dataset compatible with the Ragas framework\n",
    "    '''\n",
    "    # Ensure all text columns are strings and handle NaN values\n",
    "    text_columns = ['question', 'ground_truth', 'answer']\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].fillna('').astype(str)\n",
    "        \n",
    "    # Convert 'contexts' to a list of lists\n",
    "    df['contexts'] = df['contexts'].fillna('').astype(str).apply(lambda x: [x] if x else [])\n",
    "    \n",
    "    # Converting the DataFrame to a dictionary\n",
    "    data_dict = df[['question', 'contexts', 'answer', 'ground_truth']].to_dict('list')\n",
    "    \n",
    "    # Loading the dictionary as a Hugging Face dataset\n",
    "    ragas_testset = Dataset.from_dict(data_dict)\n",
    "    \n",
    "    return ragas_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongb\\AppData\\Local\\Temp\\ipykernel_7828\\1620957375.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna('').astype(str)\n",
      "C:\\Users\\jongb\\AppData\\Local\\Temp\\ipykernel_7828\\1620957375.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna('').astype(str)\n",
      "C:\\Users\\jongb\\AppData\\Local\\Temp\\ipykernel_7828\\1620957375.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna('').astype(str)\n",
      "C:\\Users\\jongb\\AppData\\Local\\Temp\\ipykernel_7828\\1620957375.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['contexts'] = df['contexts'].fillna('').astype(str).apply(lambda x: [x] if x else [])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'contexts', 'answer', 'ground_truth'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_testset = pandas_to_ragas(df = t_df)\n",
    "ragas_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[\"[None, None, \\'* To be conducted by a test organization or a submitter.\\\\\\\\\\\\\\\\nFigure 1: Basic flow of survey and certification process\\\\\\\\\\\\\\\\n4.2 Preliminary verification of the attained EEDI at the design stage\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n4.2.1 For the preliminary verification at the design stage, an application for an initial survey and an EEDI TechnEical File containing the necessary information for the verification and\\\\\\\\\\\\\\\\nother relevant background documents should be submitted to a verifier.\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n4.2.2 The EEDI Technical File should be written at least in English. The EEDI Technical File should include Sas a minimum, but not limited to:\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n.1 deadweight (DWT) or gross tonnage (GT) for passenger and ro-ro passenger ships, the maximuGm continuous rating (MCR) of the main and auxiliary engines, the ship speed\\\\\\\\\\\\\\\\n(Vref), as specified in paragraph 2.2 of the EEDI Calculation Guidelines, type of fuel, the specific fuel consumption (SFC) of the main engine at 75% of MCR power, the SFC of the\\']\"]']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_testset[\"contexts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "metrics=[\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    ],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval with ChatGPT(3.5 and 4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e4664098984b3f9f41615d5a7b81ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.8994, 'answer_correctness': 0.4422, 'answer_similarity': 0.8457, 'faithfulness': 0.2821, 'context_precision': 0.0000, 'context_recall': 0.0000}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# llm = ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    "llm = ChatOpenAI(model = 'gpt-4o')\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "naive_results = evaluate(\n",
    "    ragas_testset, \n",
    "    metrics = [\n",
    "        answer_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm = llm,\n",
    "    embeddings=embeddings,\n",
    "    raise_exceptions=False)\n",
    "\n",
    "naive_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.8958,\n",
       " 'answer_correctness': 0.3211,\n",
       " 'answer_similarity': 0.8741,\n",
       " 'faithfulness': 0.3141,\n",
       " 'context_precision': 0.5,\n",
       " 'context_recall': 0.35}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'answer_relevancy': 0.8958, 'answer_correctness': 0.3211, 'answer_similarity': 0.8741, 'faithfulness': 0.3141, 'context_precision': 0.5000, 'context_recall': 0.3500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval with Groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b86d3ed2b84af4a3bd12365821cf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 221, in _ascore\n",
      "    item_statement = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 57, in __call__\n",
      "    await self.sleep(do)\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 221, in _ascore\n",
      "    item_statement = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 57, in __call__\n",
      "    await self.sleep(do)\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 266, in _ascore\n",
      "    nli_result = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 57, in __call__\n",
      "    await self.sleep(do)\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 152, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 57, in __call__\n",
      "    await self.sleep(do)\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 266, in _ascore\n",
      "    nli_result = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 57, in __call__\n",
      "    await self.sleep(do)\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\evaluation.py:304: RuntimeWarning: Mean of empty slice\n",
      "  value = np.nanmean(self.scores[cn])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.7599, 'answer_correctness': nan, 'answer_similarity': 0.8926, 'faithfulness': nan, 'context_precision': 0.5000, 'context_recall': 0.5000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "# llm = ChatGroq(name=\"gemma2-9b-it\")   \n",
    "llm = ChatGroq(name=\"llama3-8b-8192\")  \n",
    "# llm = ChatGroq(name=\"llama3-70b-8192\") \n",
    "#  \n",
    "naive_results = evaluate(\n",
    "    ragas_testset, \n",
    "    metrics = [\n",
    "        answer_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm = llm,\n",
    "    embeddings=embeddings,\n",
    "    raise_exceptions=False)\n",
    "\n",
    "naive_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc34ece39914dd78dbeb17eff8701ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 66, in _ascore\n",
      "    embedding_2 = np.array(await self.embeddings.embed_text(answer))\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 26, in embed_text\n",
      "    embs = await self.embed_texts([text], is_async=is_async)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 36, in embed_texts\n",
      "    return await aembed_documents_with_retry(texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 67, in aembed_documents\n",
      "    return await self.embeddings.aembed_documents(texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\embeddings\\embeddings.py\", line 21, in aembed_documents\n",
      "    return await run_in_executor(None, self.embed_documents, texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\runnables\\config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 66, in _ascore\n",
      "    embedding_2 = np.array(await self.embeddings.embed_text(answer))\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 26, in embed_text\n",
      "    embs = await self.embed_texts([text], is_async=is_async)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 36, in embed_texts\n",
      "    return await aembed_documents_with_retry(texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 67, in aembed_documents\n",
      "    return await self.embeddings.aembed_documents(texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\embeddings\\embeddings.py\", line 21, in aembed_documents\n",
      "    return await run_in_executor(None, self.embed_documents, texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\runnables\\config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 66, in _ascore\n",
      "    embedding_2 = np.array(await self.embeddings.embed_text(answer))\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 26, in embed_text\n",
      "    embs = await self.embed_texts([text], is_async=is_async)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 36, in embed_texts\n",
      "    return await aembed_documents_with_retry(texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 67, in aembed_documents\n",
      "    return await self.embeddings.aembed_documents(texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\embeddings\\embeddings.py\", line 21, in aembed_documents\n",
      "    return await run_in_executor(None, self.embed_documents, texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\runnables\\config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 161, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 152, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_recall.py\", line 169, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 248, in _ascore\n",
      "    statements = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 152, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 221, in _ascore\n",
      "    item_statement = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 161, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_recall.py\", line 169, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 314, in _acreate_stream\n",
      "    async for line in response.content:\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 50, in __anext__\n",
      "    rv = await self.read_func()\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 317, in readline\n",
      "    return await self.readuntil()\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 351, in readuntil\n",
      "    await self._wait(\"readuntil\")\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 312, in _wait\n",
      "    await waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 248, in _ascore\n",
      "    statements = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 161, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 248, in _ascore\n",
      "    statements = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_recall.py\", line 169, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_recall.py\", line 169, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_similarity.py\", line 66, in _ascore\n",
      "    embedding_2 = np.array(await self.embeddings.embed_text(answer))\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 26, in embed_text\n",
      "    embs = await self.embed_texts([text], is_async=is_async)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 36, in embed_texts\n",
      "    return await aembed_documents_with_retry(texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\embeddings\\base.py\", line 67, in aembed_documents\n",
      "    return await self.embeddings.aembed_documents(texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\embeddings\\embeddings.py\", line 21, in aembed_documents\n",
      "    return await run_in_executor(None, self.embed_documents, texts)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\runnables\\config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 152, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 221, in _ascore\n",
      "    item_statement = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 152, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 221, in _ascore\n",
      "    item_statement = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 248, in _ascore\n",
      "    statements = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 161, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 314, in _acreate_stream\n",
      "    async for line in response.content:\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 50, in __anext__\n",
      "    rv = await self.read_func()\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 317, in readline\n",
      "    return await self.readuntil()\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 351, in readuntil\n",
      "    await self._wait(\"readuntil\")\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 312, in _wait\n",
      "    await waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_correctness.py\", line 221, in _ascore\n",
      "    item_statement = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\evaluation.py:304: RuntimeWarning: Mean of empty slice\n",
      "  value = np.nanmean(self.scores[cn])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': nan, 'answer_correctness': nan, 'answer_similarity': nan, 'faithfulness': nan, 'context_precision': nan, 'context_recall': nan}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"phi3:latest\")\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "naive_results = evaluate(\n",
    "    ragas_testset, \n",
    "    metrics = [\n",
    "        answer_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm = llm,\n",
    "    embeddings=embeddings,\n",
    "    raise_exceptions=False)\n",
    "\n",
    "naive_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
