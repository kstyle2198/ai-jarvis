{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = ['What were the challenges faced with the previous versioning policy of langchain and how will the new versioning standard address them?']\n",
    "answers = [' The challenges faced with the previous versioning policy of langchain were that users couldn\\'t be confident in updates not having breaking changes, and the framework became bloated and unstable due to a \"maintain everything\" approach. The new versioning standard will address these challenges by clearly communicating any breaking changes, providing an avenue for deprecating and deleting old code, and more responsibly dealing with integrations.']\n",
    "contexts = ['[\"versioning policy for a little over a month now.langchain itself, however, still remained on 0.0.x versions. Having all releases on minor version 0 created a few challenges:Users couldn‚Äôt be confident that updating would not have breaking changeslangchain became bloated and unstable as we took a ‚Äúmaintain everything‚Äù approach to reduce breaking changes and deprecation notificationsHowever, starting today with the release of langchain 0.1.0, all future releases will follow a new versioning standard. Specifically:Any breaking changes to the public API will result in a minor version bump (the second digit)Any bug fixes or new features will result in a patch version bump (the third digit)We hope that this, combined with the previous architectural changes, will:Communicate clearly if breaking changes are made, allowing developers to update with confidenceGive us an avenue for officially deprecating and deleting old code, reducing bloatMore responsibly deal with integrations (whose SDKs are often changing as rapidly as LangChain)Even after we release a 0.2 version, we will commit to maintaining a branch of 0.1, but will only patch critical bug fixes. See more towards the end of this post on our plans for that.While\"]']\n",
    "test_groundtruths = [\"The challenges faced with the previous versioning policy of langchain were that users couldn't be confident that updating would not have breaking changes and langchain became bloated and unstable. The new versioning standard will address these challenges by ensuring that any breaking changes to the public API will result in a minor version bump and any bug fixes or new features will result in a patch version bump.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What were the challenges faced with the previo...</td>\n",
       "      <td>The challenges faced with the previous versio...</td>\n",
       "      <td>[\"versioning policy for a little over a month ...</td>\n",
       "      <td>The challenges faced with the previous version...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What were the challenges faced with the previo...   \n",
       "\n",
       "                                              answer  \\\n",
       "0   The challenges faced with the previous versio...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [\"versioning policy for a little over a month ...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  The challenges faced with the previous version...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"question\": test_questions, \"answer\": answers, \"contexts\": contexts, \"ground_truth\": test_groundtruths})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   question      1 non-null      object\n",
      " 1   answer        1 non-null      object\n",
      " 2   contexts      1 non-null      object\n",
      " 3   ground_truth  1 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 160.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"contexts\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   question      1 non-null      object\n",
      " 1   answer        1 non-null      object\n",
      " 2   contexts      1 non-null      object\n",
      " 3   ground_truth  1 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 160.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "for col in [\"contexts\"]:\n",
    "    df[col] = df[col].apply(ast.literal_eval)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What were the challenges faced with the previous versioning policy of langchain and how will the new versioning standard address them?',\n",
       " 'answer': ' The challenges faced with the previous versioning policy of langchain were that users couldn\\'t be confident in updates not having breaking changes, and the framework became bloated and unstable due to a \"maintain everything\" approach. The new versioning standard will address these challenges by clearly communicating any breaking changes, providing an avenue for deprecating and deleting old code, and more responsibly dealing with integrations.',\n",
       " 'contexts': ['versioning policy for a little over a month now.langchain itself, however, still remained on 0.0.x versions. Having all releases on minor version 0 created a few challenges:Users couldn‚Äôt be confident that updating would not have breaking changeslangchain became bloated and unstable as we took a ‚Äúmaintain everything‚Äù approach to reduce breaking changes and deprecation notificationsHowever, starting today with the release of langchain 0.1.0, all future releases will follow a new versioning standard. Specifically:Any breaking changes to the public API will result in a minor version bump (the second digit)Any bug fixes or new features will result in a patch version bump (the third digit)We hope that this, combined with the previous architectural changes, will:Communicate clearly if breaking changes are made, allowing developers to update with confidenceGive us an avenue for officially deprecating and deleting old code, reducing bloatMore responsibly deal with integrations (whose SDKs are often changing as rapidly as LangChain)Even after we release a 0.2 version, we will commit to maintaining a branch of 0.1, but will only patch critical bug fixes. See more towards the end of this post on our plans for that.While'],\n",
       " 'ground_truth': \"The challenges faced with the previous versioning policy of langchain were that users couldn't be confident that updating would not have breaking changes and langchain became bloated and unstable. The new versioning standard will address these challenges by ensuring that any breaking changes to the public API will result in a minor version bump and any bug fixes or new features will result in a patch version bump.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tds = Dataset.from_pandas(df)\n",
    "tds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d47c6490704eae834d43bbc4f18a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 314, in _acreate_stream\n",
      "    async for line in response.content:\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 50, in __anext__\n",
      "    rv = await self.read_func()\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 317, in readline\n",
      "    return await self.readuntil()\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 351, in readuntil\n",
      "    await self._wait(\"readuntil\")\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 312, in _wait\n",
      "    await waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 248, in _ascore\n",
      "    statements = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_recall.py\", line 169, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 152, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 323, in _agenerate\n",
      "    final_chunk = await self._achat_stream_with_aggregation(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 244, in _achat_stream_with_aggregation\n",
      "    async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 203, in _acreate_chat_stream\n",
      "    async for stream_resp in self._acreate_stream(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 294, in _acreate_stream\n",
      "    async with session.post(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 1197, in __aenter__\n",
      "    self._resp = await self._coro\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client.py\", line 608, in _request\n",
      "    await resp.start(conn)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 976, in start\n",
      "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\aiohttp\\streams.py\", line 640, in read\n",
      "    await self._waiter\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 285, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 234, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 161, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"D:\\python\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 535, in agenerate\n",
      "    results = await asyncio.gather(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 304, in __wakeup\n",
      "    future.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "  File \"D:\\python\\lib\\asyncio\\futures.py\", line 196, in result\n",
      "    raise exc\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "  File \"D:\\python\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "d:\\ai_jarvis\\jarvis_env\\lib\\site-packages\\ragas\\evaluation.py:304: RuntimeWarning: Mean of empty slice\n",
      "  value = np.nanmean(self.scores[cn])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_precision': nan, 'faithfulness': nan, 'answer_relevancy': nan, 'context_recall': nan}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"phi3:latest\")\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "naive_results = evaluate(\n",
    "    tds, \n",
    "    metrics = [\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm = llm,\n",
    "    embeddings=embedding_model,\n",
    "    raise_exceptions=False)\n",
    "\n",
    "naive_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
